
# Ex.No: 2 	Evaluation of 2024 Prompting Tools Across Diverse AI Platforms: ChatGPT, Claude, Bard, Cohere Command, and Meta 

# DATE:<br />                                                             REGISTER NUMBER : 212222060091
 
### Aim:
To compare the performance, user experience, and response quality of different AI platforms (ChatGPT, Claude, Bard, Cohere Command, and Meta) within a specific use case, such as summarizing text or answering technical questions. Generate a Prompt based output using different Prompting tools of 2024.
### AI Tools required:
ChatGPT (GPT-4) → chat.openai.com<br />
Claude (Anthropic) → claude.ai<br />
Bard (Gemini) → bard.google.com<br />
Cohere Command R+ → cohere.com<br />
Meta LLaMA (via HuggingFace or local deployment<br />

### Explanation:
# Define the Use Case:<br />
Select a specific task for evaluation across platforms (e.g., summarizing a document, answering a technical question, or generating a creative story / Code).<br />
Ensure the use case is applicable to all platforms and will allow for comparison across response quality, accuracy, and depth.<br />
# Create a Set of Prompts:<br />
Prepare a uniform set of prompts that align with the chosen use case.
Each prompt should be clear and precise, ensuring that all platforms are evaluated using the same input.<br />
Consider multiple prompts to capture the versatility of each platform in handling different aspects of the use case.<br />
# Run the Experiment on Each AI Platform:<br />
Input the prompts into each AI tool (ChatGPT, Claude, Bard, Cohere Command, and Meta) and gather the responses.<br />
Ensure the same conditions are applied for each platform, such as input format, time to respond, and prompt delivery.<br />
Record response times, ease of interaction with the platform, and any technical issues encountered.<br />
# Evaluate Response Quality:<br />
Assess each platform’s responses using the following criteria: Accuracy,Clarity,Depth,Relevance <br />
# Compare Performance:<br />
Compare the collected data to identify differences in performance across platforms.<br />
Identify any platform-specific advantages, such as faster response times, more accurate answers, or more intuitive interfaces.<br />
# Deliverables:<br />
A comparison table outlining the performance of each platform (ChatGPT, Claude, Bard, Cohere Command, and Meta) based on accuracy, clarity, depth, and relevance of responses.<br />
A final report summarizing the findings of the experiment, including recommendations on the most suitable AI platform for different use cases based on performance and user <br />

### Output:

# Prompt Types Explained (Use Case: Travel Itinerary Generation)
# 1. Informative Prompt
# Prompt:
```
“What are the top 5 places to visit in Kyoto, Japan, and why?”
```
ChatGPT: Clear, informative, strong cultural context.<br />
Claude: In-depth with historical relevance.<br />
Bard: Accurate with links and map info.<br />
Cohere: Lists only basic places, lacks depth.<br />
Meta LLaMA: Generic, less tourist-friendly.<br />

# 2. Creative Prompt
# Prompt:
```
 “Create a 3-day romantic travel plan for a couple visiting Paris.”
```
ChatGPT: Beautiful, well-structured plan with emotional touch.<br />
Claude: Poetic and immersive experience.<br />
Bard: Practical but less imaginative.<br />
Cohere: Basic and utilitarian.<br />
Meta LLaMA: Disjointed, lacks narrative flow.<br />

# 3. Instruction-following Prompt
# Prompt:
```
“List 4 travel tips for someone visiting Iceland in winter.”
```
ChatGPT: Clear and accurate, perfectly formatted.<br />
Claude: Slightly verbose but detailed.<br />
Bard: Precise, sometimes adds a 5th tip.<br />
Cohere: Misses formatting consistency.<br />
Meta LLaMA: May skip instructions.<br />

# 4. Logical Reasoning Prompt<br />
# Prompt:
 ```
“If someone wants to avoid crowds, which months are best to visit Italy?”
```
ChatGPT: Correct logic with justification (shoulder seasons).<br />
Claude: Same answer with more context and examples.<br />
Bard: Technically accurate, brief.<br />
Cohere: Unclear seasonal understanding.<br />
Meta LLaMA: Vague response.<br />

# 5. Conversational Prompt
# Prompt:
```
 “I’m feeling overwhelmed planning a vacation. Can you help me get started?”
```
ChatGPT: Supportive, structured advice.<br />
Claude: Empathetic and friendly tone.<br />
Bard: Functional but slightly cold.<br />
Cohere: Robotic and directive.<br />
Meta LLaMA: Technical and impersonal.<br />

# 6. Code Prompt (New)
# Prompt:
```
“Write a Python script that lists top-rated tourist attractions using user-provided city name (use dummy data).”
```
ChatGPT: Clear code with comments.<br />
Claude: Functional, well-structured.<br />
Bard: Works well but lacks comments.<br />
Cohere: Runs fine but lacks error handling.<br />
Meta LLaMA: Often incomplete.<br />

### Conclusion: 
In the travel planning use case, ChatGPT and Claude performed best in offering structured, emotionally aware, and user-friendly outputs. Bard was technically useful, especially for data-rich queries. Cohere is functional but lacked depth and emotional engagement. Meta LLaMA, while promising, is still more suited for structured or developer-centric tasks.

# Result :
Thus, the evaluation of prompting tools across leading AI platforms—ChatGPT, Claude, Bard, Cohere Command, and Meta LLaMA—has been successfully analyzed in the context of travel itinerary generation.
